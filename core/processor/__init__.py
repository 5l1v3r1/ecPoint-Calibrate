import os
from datetime import datetime, timedelta
from textwrap import dedent

from core.loaders.ascii import ASCIIEncoder
from core.loaders.fieldset import Fieldset
from core.loaders.geopoints import Geopoints

from ..computations.models import Computation
from .utils import (
    adjust_leadstart,
    compute_local_solar_time,
    generate_steps,
    iter_daterange,
    log,
)


def run(config):
    BaseDateS = config.parameters.date_start
    BaseDateF = config.parameters.date_end
    Acc = config.predictand.accumulation
    LimSU = config.parameters.limit_spin_up
    Range = config.parameters.leadstart_range
    PathOBS = config.observations.path
    PathFC = config.predictors.path
    PathPredictand = config.predictand.path
    PathOUT = config.parameters.out_path

    # Set up the input/output parameters
    BaseDateS = datetime.strptime(BaseDateS, "%Y%m%d").date()
    BaseDateF = datetime.strptime(BaseDateF, "%Y%m%d").date()
    BaseDateSSTR = BaseDateS.strftime("%Y%m%d")
    BaseDateFSTR = BaseDateF.strftime("%Y%m%d")
    AccSTR = f"Acc{Acc:02}h"

    computations = config.computations.fields

    serializer = ASCIIEncoder(path=PathOUT)

    header = dedent(
        f"""
        # THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
        #
        # Created on {datetime.now()}.
        #
        # Forecast Verification for HRES
        #
        # Parameters:
        #     Model Type          = {config.parameters.model_type}
        #     Base date start     = {BaseDateSSTR}
        #     Base date end       = {BaseDateFSTR}
        #     Spin-up limit       = {LimSU}h
        #     Range               = 1
        #
        # Predictand:
        #     Short code          = {config.predictand.code}
        #     Error               = {config.predictand.error}
        #     Accumulation        = {config.predictand.accumulation}
        #     Minimum value       = {config.predictand.min_value}
        #
        """
    )

    codes_to_compute = [
        computation["shortname"]
        for computation in computations
        if computation["isPostProcessed"]
    ]

    header += "# Post-processed computations: " + ", ".join(codes_to_compute)

    serializer.header = header.strip()

    #############################################################################################

    # PROCESSING MODEL DATA
    yield log.info(
        "****************************************************************************************************"
    )
    yield log.info("POST-PROCESSING SOFTWARE TO PRODUCE FORECASTS AT POINTS - ecPoint")
    yield log.info(
        "The user is running the ecPoint-RAINFALL family, Operational Version 1"
    )
    yield log.info(
        f"Forecast Error Ratio (FER) and Predictors for {Acc}  hour accumulation."
    )
    yield log.info("List of predictors:")
    yield log.info(
        "- Convective precipitation ratio, cpr = convective precipitation / total precipitation [-]"
    )
    yield log.info(f"- Total precipitation, tp [mm/{Acc}h]")
    yield log.info("- Wind speed of steering winds (at 700 mbar), wspd700 [m/s]")
    yield log.info("- Convective available potential energy, cape [J/kg]")
    yield log.info("- Daily accumulation of clear-sky solar radiation, sr24h [W/m2]")
    yield log.info("- Local Solar Time, lst [hours]")
    yield log.info(
        "****************************************************************************************************"
    )

    # Counter for the BaseDate and BaseTime to avoid repeating the same forecasts in different cases
    counterValidTimes = set()
    obsTOT = 0
    obsUSED = 0

    for curr_date, curr_time, leadstart in iter_daterange(BaseDateS, BaseDateF, model_runs_per_day=2, leadstart_increment=Range):
        yield log.info("FORECAST PARAMETERS")
        yield log.info(
            "BaseDate={} BaseTime={:02d} UTC (t+{}, t+{})".format(
                curr_date.strftime("%Y%m%d"), curr_time, leadstart, leadstart + Acc
            )
        )

        curr_date, curr_time, leadstart = adjust_leadstart(
            date=curr_date,
            hour=curr_time,
            leadstart=leadstart,
            limSU=LimSU,
            model_runs_per_day=2,
        )
        thedateNEWSTR = curr_date.strftime("%Y%m%d")
        thetimeNEWSTR = f"{curr_time:02d}"

        yield log.info(
            f"BaseDate={thedateNEWSTR} BaseTime={thetimeNEWSTR} UTC (t+{leadstart}, t+{leadstart + Acc})"
        )

        # Reading the forecasts
        if curr_date < BaseDateS or curr_date > BaseDateF:
            log.warn(
                f"Requested date {curr_date} outside input date range: {BaseDateSSTR} - {BaseDateFSTR}"
            )
            continue

        def get_grib_path(predictand, step):
            return os.path.join(
                PathFC,
                predictand,
                thedateNEWSTR + thetimeNEWSTR,
                "_".join([predictand, thedateNEWSTR, thetimeNEWSTR, f"{step:02d}"])
                + ".grib",
            )

        # Note about the computation of the sr.
        # The solar radiation is a cumulative variable and its units is J/m2 (which means, W*s/m2).
        # One wants the 24h. The 24h mean is obtained by taking the difference between the beginning and the end of the 24 hourly period
        # and dividing by the number of seconds in that period (24h = 86400 sec). Thus, the unit will be W/m2

        steps = [leadstart + step for step in generate_steps(Acc)]

        # Defining the parameters for the rainfall observations
        validDateF = (
            datetime.combine(curr_date, datetime.min.time())
            + timedelta(hours=curr_time)
            + timedelta(hours=steps[-1])
        )
        DateVF = validDateF.strftime("%Y%m%d")
        HourVF = validDateF.strftime("%H")
        HourVF_num = validDateF.hour
        yield log.info("RAINFALL OBS PARAMETERS")
        yield log.info(
            f"Validity date/time (end of {Acc} hourly " f"period) = {validDateF}"
        )

        # Looking for no repetions in the computed dates and times
        if validDateF in counterValidTimes:
            yield log.warn("Valid Date and Time already computed.")
            continue

        counterValidTimes.add(validDateF)
        dirOBS = os.path.join(PathOBS, AccSTR, DateVF)
        fileOBS = f"tp_{Acc:02d}_{DateVF}_{HourVF}.geo"

        obs_path = os.path.join(dirOBS, fileOBS)

        # Reading Rainfall Observations
        yield log.info(f"Read observation: {obs_path}")
        try:
            obs = Geopoints.from_path(path=obs_path)
        except IOError:
            yield log.warn(f"File not found in DB: {obs_path}.")
            continue
        except Exception:
            yield log.error(f"Error reading observation file {obs_path}")
            continue

        nOBS = len(obs.dataframe)

        if nOBS <= 1:
            # which will account for the cases of zero observation in the geopoint file (because the length of the vector will be forced to 1),
            # or cases in which there is only one observation in the geopoint file
            yield log.warn(f"No observations: {obs_path}.")
            continue

        obsTOT += nOBS
        if steps[-1] <= 24:
            step_start_sr, step_end_sr = 1, 25
        else:
            step_start_sr, step_end_sr = steps[-1] - 24, steps[-1]

        yield log.info("Read forecast data")

        yield log.info(
            f"Computing the following parameters: {', '.join(codes_to_compute)}"
        )

        for computation in computations:
            computation["isReference"] = (
                len(computation["inputs"]) == 1
                and computation["inputs"][0] == config.predictand.code
            )

        base_fields = set(config.predictors.codes)

        derived_computations = [
            computation
            for computation in computations
            if set(computation["inputs"]) - base_fields != set()
        ]

        base_computations = sorted(
            [
                computation
                for computation in computations
                if computation not in derived_computations
            ],
            key=lambda computation: computation["isReference"],
            reverse=True,
        )

        computations_cache = {}
        computations_result = []
        skip = False

        for computation in base_computations:
            computer = Computation(computation)
            predictor_code = computer.computation["inputs"][0]

            steps = (
                [step_start_sr, step_end_sr]
                if computation["field"] == "24H_SOLAR_RADIATION"
                else steps
            )

            try:
                computation_steps = [
                    Fieldset.from_path(path=get_grib_path(predictor_code, step))
                    for step in steps
                ]
            except (IOError, Exception):
                skip = True
                break

            computed_value = computer.run(*computation_steps)

            computations_cache[computation["shortname"]] = computed_value

            if not computation["isReference"] and not computation["isPostProcessed"]:
                continue

            yield log.info("Selecting the nearest grid point to rainfall observations.")
            geopoints = computed_value.nearest_gridpoint(obs)

            yield log.info(f"Selecting values that correspond to tp >= 1 mm/{Acc}h.")

            # Select only the values that correspond to TP>=1
            if computation["isReference"]:
                reference_predictor = computation["shortname"]
                ref_geopoints = geopoints
                mask = ref_geopoints.values >= 1
                ref_geopoints_filtered_df = ref_geopoints.dataframe[mask]

                if ref_geopoints_filtered_df.empty:
                    yield log.warn(
                        f"No values of {computation['shortname']} >= 1 mm/{Acc}h."
                    )
                    skip = True
                    break
                elif computation["isPostProcessed"]:
                    computations_result.append(
                        (computation["shortname"], ref_geopoints_filtered_df["value"])
                    )
            else:
                geopoints_filtered_df = geopoints.dataframe[mask]

                computations_result.append(
                    (computation["shortname"], geopoints_filtered_df["value"])
                )

        if skip:
            continue

        derived_computations = [
            computation
            for computation in derived_computations
            if computation["isPostProcessed"]
        ]

        for computation in derived_computations:
            computer = Computation(computation)
            steps = [
                computations_cache[field_input] for field_input in computation["inputs"]
            ]

            if computation["field"] == "RATIO_FIELD":
                dividend = steps[0]
                # [TODO] Cache the following in the computations_cache
                geopoints = dividend.nearest_gridpoint(obs)
                geopoints_filtered_df = geopoints.dataframe[mask]

                computed_value = computer.run(
                    geopoints_filtered_df["value"], ref_geopoints_filtered_df["value"]
                )
                computations_result.append((computation["shortname"], computed_value))
            else:
                computed_value = computer.run(*steps)
                geopoints = computed_value.nearest_gridpoint(obs)
                geopoints_filtered_df = geopoints.dataframe[mask]
                computations_result.append(
                    (computation["shortname"], geopoints_filtered_df["value"])
                )

        # Compute other parameters
        obs1 = obs.dataframe[mask]

        latObs_1 = obs1["latitude"]
        lonObs_1 = obs1["longitude"]
        # [XXX] CPr = CP_Ob1 / TP_Ob1

        vals_errors = []
        if config.predictand.error == "FER":
            FER = (
                obs1["value"] - ref_geopoints_filtered_df["value"]
            ) / ref_geopoints_filtered_df["value"]
            vals_errors.append(("FER", FER))

        if config.predictand.error == "FE":
            FE = obs1["value"] - ref_geopoints_filtered_df["value"]
            vals_errors.append(("FE", FE))

        vals_LST = compute_local_solar_time(longitudes=lonObs_1, hour=HourVF_num)

        # Saving the output file in ascii format
        vals_OB = obs1["value"]

        n = len(vals_OB)
        obsUSED = obsUSED + n
        yield log.success(f"Write data to: {PathOUT}")

        columns = (
            [
                ("Date", [DateVF] * n),
                ("TimeUTC", [HourVF] * n),
                ("OBS", vals_OB),
                ("latOBS", latObs_1),
                ("lonOBS", lonObs_1),
                ("LST", vals_LST),
            ]
            + vals_errors
            + computations_result
        )

        serializer.add_columns_chunk(columns)

        yield log.info("\n" + "*" * 80)

    yield log.success(f"Number of observations in the whole training period: {obsTOT}")
    yield log.success(
        f"Number of observations actually used in the training period "
        f"(tp >= 1 mm/{Acc}h): {obsUSED}"
    )

    footer = dedent(
        f"""
        # Number of observations in the whole training period = {obsTOT}
        # Number of observations actually used in the training period (corresponding to {reference_predictor} => 1mm/{Acc}h) = {obsUSED}
        """
    ).strip()
    serializer.footer = footer
    serializer.write()
